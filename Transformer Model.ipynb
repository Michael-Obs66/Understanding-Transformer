{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[263, 395, 755, 384,  59]])\n",
      "Output shape: torch.Size([1, 5, 1000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Transformer Encoder Definition\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, ff_dim, num_layers, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        # Embedding layer to convert input tokens into numerical vectors\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        \n",
    "        # Define a single Transformer Encoder layer with batch_first=True for better performance\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,  # Dimension of embedding vector\n",
    "            nhead=num_heads,  # Number of attention heads (multi-head attention)\n",
    "            dim_feedforward=ff_dim,  # Hidden layer size in Feedforward Network (FFN)\n",
    "            dropout=dropout,  # Dropout to prevent overfitting\n",
    "            batch_first=True  # Ensure batch is the first dimension for efficient processing\n",
    "        )\n",
    "        \n",
    "        # Transformer Encoder with multiple layers\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Linear layer to process the final encoder output\n",
    "        self.fc = nn.Linear(embed_dim, input_dim)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Convert input tokens into embeddings\n",
    "        src = self.embedding(src)\n",
    "        \n",
    "        # Pass through the Transformer Encoder\n",
    "        src = self.transformer_encoder(src)\n",
    "        \n",
    "        # Pass through fully connected layer to produce final output\n",
    "        output = self.fc(src)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 1000  # Vocabulary size\n",
    "embed_dim = 64    # Embedding dimension\n",
    "num_heads = 8     # Number of attention heads in multi-head attention\n",
    "ff_dim = 256      # Hidden layer size in Feedforward Network (FFN)\n",
    "num_layers = 3    # Number of encoder layers\n",
    "\n",
    "# Initialize model\n",
    "model = TransformerEncoder(input_dim, embed_dim, num_heads, ff_dim, num_layers)\n",
    "\n",
    "# Sample input with batch_first=True (batch_size=1, sequence_length=5)\n",
    "sample_input = torch.randint(0, input_dim, (1, 5))  # (batch_size=1, sequence_length=5)\n",
    "output = model(sample_input)  # Perform forward pass\n",
    "\n",
    "# Display results\n",
    "print(\"Input:\", sample_input)\n",
    "print(\"Output shape:\", output.shape)  # Expected shape: (batch_size, sequence_length, input_dim)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
